{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f-ewnQs55UL"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, FloatSlider, IntSlider\n",
    "import ipywidgets as widgets\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ex0Ntnw3tNDl"
   },
   "source": [
    "## Question 3) Kernel PCA\n",
    "\n",
    "In previous assignments, you've worked with PCA to find a lower dimensional representation of a data matrix, allowing us to perform tasks like classification more easily.  \n",
    "As we said in the notes, kernels have a wide range of applications. In this problem, we'll take a look at the application of kernels to PCA.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpLooZAsI6yF"
   },
   "source": [
    "### Question 3a)\n",
    "First, let's look at the half moon data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "INaVOdlxI6SG",
    "outputId": "5bce54e3-9bf0-4b0b-93e9-ec28c92eae60"
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples = 500, noise = 0.04) \n",
    "plt.scatter(X[:, 0], X[:, 1], c = y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmIcCxvTJ8ks"
   },
   "source": [
    "Run PCA on this dataset with 1 and 2 components, and visualize the result. Fill in the code such that `X_red` has the original data projected onto the first 2 principal components. Answer the following questions.\n",
    "* **Do you notice anything different about this graph? Why did this change happen?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "a1oEQkPDJFse",
    "outputId": "6136fd48-53a2-4ba1-f372-87fd44bec675"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18,4))\n",
    "# START TODO\n",
    "\n",
    "# END TODO\n",
    "axes[0].scatter(X_red[:,0], np.zeros(X_red.shape[0]), c = y) \n",
    "axes[1].scatter(X_red[:,0], X_red[:,1], c = y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA, while it is able to identify the important directions in our data, is confined to a linear feature space. This means that we are still stuck with the problem where our dataset is linearly inseparable. As we know, it often helps to lift our features by mapping each datapoint to a higher dimensional space. Kernels will allow us to lift our features without ever having to explicitly compute the higher dimensional space for our data matrix. Instead, we can simply just perform PCA on the Gram matrix K, which will give us the most important directions in this lifted feature space without having to go through the computational complexity of computing it.  \n",
    "sk-learn has a built in Kernel PCA implementation that we can use on our half moon dataset here https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html  \n",
    "Complete the following code to finish the function `kernel_pca_poly`, which takes in a value for gamma `g` and computes a 2 component KernelPCA with gamma `g` and the RBF kernel on the data in `X`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eBSd8azLJh7"
   },
   "outputs": [],
   "source": [
    "def kernel_pca_gamma(gamma):\n",
    "    \n",
    "    # START TODO\n",
    "\n",
    "    # END TODO\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18,4))\n",
    "    axes[0].scatter(X_red_kernel[:,0], np.zeros(X_red_kernel.shape[0]), c = y) \n",
    "    axes[1].scatter(X_red_kernel[:,0], X_red_kernel[:,1], c = y) \n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following code to visualize our results. Play with the gamma parameter and answer the following questions.\n",
    "* **What does the graph look like as gamma approaches infinity? What about negative infinity?**\n",
    "* **What happens when gamma is 0? Why does this make sense?** \n",
    "* **What is the value of gamma that visually seems like it would cause the data to be most separable?**\n",
    "* **What method can we use to find an optimal gamma to make this data separable?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rsdMu8yLaGA"
   },
   "outputs": [],
   "source": [
    "g_widget = FloatSlider(min=-10, max=20.0)\n",
    "\n",
    "interact(kernel_pca_gamma,gamma=g_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3b)\n",
    "Fill in the code to use a polynomial kernel now, and answer the following questions.\n",
    "* **Try keeping the degree fixed and changing gamma. What do you notice happens as gamma gets to be a large positive number? What about to be a small negative number? What about 0?** \n",
    "* **Now keep the gamma fixed and change the degree. What do you notice happens as the degree takes on even and odd values? What about small? What about 0?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_pca_poly(degree, gamma):\n",
    "    \n",
    "    # START TODO\n",
    "\n",
    "    # END TODO\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18,4))\n",
    "    axes[0].scatter(X_red_kernel[:,0], np.zeros(X_red_kernel.shape[0]), c = y) \n",
    "    axes[1].scatter(X_red_kernel[:,0], X_red_kernel[:,1], c = y) \n",
    "\n",
    "    plt.show()\n",
    "g_widget = FloatSlider(min=-10, max=20.0)\n",
    "d_widget = IntSlider(min=1, max=10)\n",
    "interact(kernel_pca_poly,gamma=g_widget,degree=d_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3c)\n",
    "Now let's do some classification with Logistic Regression to see how well we can classify the the original dataset, the dataset projected onto the first two principal components, and the dataset projected using the principal components from kernel PCA. \n",
    "\n",
    "First, let's look at the original dataset. Fill in the code in TODO such that we fit a Logistic Regression model and store the weights in a variable called `w`. Calculate the accuracy of the classifier on the dataset and store that in orig_accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO\n",
    "\n",
    "# END TODO\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y) \n",
    "ax = plt.gca()\n",
    "ax.autoscale(False)\n",
    "x_vals = np.array(ax.get_xlim())\n",
    "y_vals = -(x_vals * w[0])/w[1]\n",
    "plt.plot(x_vals, y_vals, '--', c=\"red\")\n",
    "print (\"Classifier accuracy: \", orig_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for the `X_red` dataset. Answer the following questions.\n",
    "* **Is the accuracy different?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO\n",
    "\n",
    "# END TODO\n",
    "plt.scatter(X_red[:,0], X_red[:,1], c = y) \n",
    "ax = plt.gca()\n",
    "ax.autoscale(False)\n",
    "x_vals = np.array(ax.get_xlim())\n",
    "y_vals = -(x_vals * w[0])/w[1]\n",
    "plt.plot(x_vals, y_vals, '--', c=\"red\")\n",
    "print (\"Classifier accuracy: \", pca_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use kernel PCA. Use PCA with an RBF kernel to transform the dataset and use the value for gamma you visually identified in Question 2b. \n",
    "* **How's the accuracy this time?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO\n",
    "\n",
    "\n",
    "# END TODO\n",
    "plt.scatter(X_red_kernel[:,0], X_red_kernel[:,1], c = y) \n",
    "ax = plt.gca()\n",
    "ax.autoscale(False)\n",
    "x_vals = np.array(ax.get_xlim())\n",
    "y_vals = -(x_vals * w[0])/w[1]\n",
    "plt.plot(x_vals, y_vals, '--', c=\"red\")\n",
    "print (\"Classifier accuracy: \", kpca_accuracy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "kernel_pca.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
