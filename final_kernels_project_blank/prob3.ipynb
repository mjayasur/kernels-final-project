{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.datasets import load_digits\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, FloatSlider, IntSlider\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Perceptron\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3) Kernel Perceptrons\n",
    "Previously, you've used perceptrons for binary classification problems. We've already seen in this assignment that we can apply kernels to PCA, but the potential of kernels is limitless. In this question, we'll kernalize perceptrons to express richer decision boundaries than the simple decision boundaries you've seen thus far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3a)\n",
    "Run the following code to visualize our dataset, and answer the following questions.\n",
    "* **Is the data linearly separable?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=400, factor=.3, noise=.12)\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to fit a perceptron to this data. Fill in the following code to fit a Perceptron classifier `clf` to X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO\n",
    "clf = ...\n",
    "# END TODO\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\"Perceptron accuracy on original data: \" + str(clf.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3b)\n",
    "Now let's try lifting the data to a higher dimensional feature space so that we can classify this with a Perceptron better. Let's apply the following feature map:\n",
    "\\begin{equation*}\n",
    "\\phi (x) = [x_1, x_2, x_1^2 + x_2^2]\n",
    "\\end{equation*}\n",
    "Fill in the following code to create a data matrix `phi` that contains our augmented data, and visualize phi.   \n",
    "**Hints:**  \n",
    "* Check out np.column_stack https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html#numpy.column_stack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO\n",
    "phi = ...\n",
    "# END TODO\n",
    "\n",
    "fig = plt.figure(figsize=(11,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(phi[:,0],phi[:,1],phi[:,2], c=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions.\n",
    "* **Is the data separable now?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try using the perceptron on this augmented dataset, and see how it performs. This is a little janky, but you can use the sliders to control the view of the 3d plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_perceptron(x_rotate, y_rotate):\n",
    "    # START TODO\n",
    "    \n",
    "    clf = ...\n",
    "    \n",
    "    # START TODO\n",
    "    \n",
    "    print (\"Perceptron accuracy on lifted data: \" + str(clf.score(phi, y)))\n",
    "\n",
    "    z = lambda x,y: (-clf.intercept_[0]-clf.coef_[0][0]*x -clf.coef_[0][1]*y) / clf.coef_[0][2]\n",
    "\n",
    "    tmp = np.linspace(-5,5,30)\n",
    "    x_surf,y_surf = np.meshgrid(tmp,tmp)\n",
    "\n",
    "    fig = plt.figure(figsize=(11,8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(phi[:,0],phi[:,1],phi[:,2], c=y)\n",
    "    ax.plot_surface(x_surf, y_surf, z(x_surf,y_surf))\n",
    "    ax.view_init(y_rotate, x_rotate)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "x_widget = IntSlider(min=-100, max=100)\n",
    "y_widget = IntSlider(min=-100, max=100)\n",
    "interact(viz_perceptron,x_rotate=x_widget,y_rotate=y_widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that lifting our features can be very advantageous to making a perceptron work. At this point this should scream kernels to you. Whenever we want to work in high dimensional spaces, a kernel would certainly help. So how do we kernelize the perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3c)\n",
    "In order to kernelize perceptrons, we need to think about about how perceptrons actually work in the first place. Fill in the following code for a homemade `MyPerceptron` class that trains on training data for `num_iterations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    if y[i] == 0:\n",
    "\n",
    "        y[i] = -1\n",
    "\n",
    "        \n",
    "class MyPerceptron:\n",
    "    def __init__(self, num_iterations = 5):\n",
    "        self.num_iterations = num_iterations\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # START TODO\n",
    "\n",
    "        # END TODO\n",
    "    def fit(self, X, y):\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            for x_i, y_i in zip(X, y):\n",
    "                # START TODO\n",
    "\n",
    "                # END TODO\n",
    "    def score(self, X, y):\n",
    "        # START TODO\n",
    "\n",
    "        # END TODO\n",
    "        accuracy = correct / X.shape[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that this works by testing classification accuracy on the original dataset, it should be around 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MyPerceptron(10)\n",
    "X_with_ones = np.column_stack([np.ones(X.shape[0]), X[:,0],X[:,1]])\n",
    "clf.fit(X_with_ones, y)\n",
    "print(\"Accuracy of MyPerceptron: \", clf.score(X_with_ones,y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at where the kernel perceptron algorithm comes from.  \n",
    "*Note: for more information visit the wikipedia page as well: https://en.wikipedia.org/wiki/Kernel_perceptron*\n",
    "* **To proceed with kernalizing this, let's first write out our update step.** \\begin{equation} w + y_i * x_i \\end{equation}\n",
    "* **Notice that w is a linear combination of our data points. w starts at zero, and only changes by getting some portion of all our data points.** (If this doesn't make sense to you read the notes once more).\n",
    "* **This means that we can write our w as follows.** \\begin{equation} w = \\sum_{i}^{n} \\alpha_i y_i \\vec{x_i}  \\end{equation}\n",
    "* **Now let's write our actual prediction formula for a given datapoint x.**\n",
    "\\begin{equation} sign(w^T x) = sign((\\sum_{i}^{n} \\alpha_i y_i \\vec x_i)^T \\vec x) = sign(\\sum_{i}^{n} \\alpha_i y_i (\\vec x_i \\cdot \\vec x))\\end{equation}\n",
    "* **Notice now that we can see an inner product in our formula for the prediction. We can replace that with a kernel function.**\n",
    "\\begin{equation} sign(w^T x) = sign((\\sum_{i}^{n} \\alpha_i y_i \\vec x_i)^T \\vec x) = sign(\\sum_{i}^{n} \\alpha_i y_i k(\\vec x_i, \\vec x))\\end{equation}\n",
    "\n",
    "* **You can see how the kernel function has worked its way into the prediction function. To perform our update step, we are no longer working with the weights `w`. We are now working with the weights $\\alpha$ instead, so we can perform this update step:** \\begin{equation} a = a + 1 \\end{equation} if a point is misclassified.\n",
    "\n",
    "Now that we understand the update step and predict formula, let's implement this regression by filling  out `MyKernelPerceptron`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyKernelPerceptron:\n",
    "    def __init__(self, kernel_function, num_iterations = 5):\n",
    "        self.num_iterations = num_iterations\n",
    "        self.k = kernel_function\n",
    "    def predict(self, y, X, x):\n",
    "        # START TODO\n",
    "\n",
    "        # END TODO\n",
    "    def fit(self, X, y):\n",
    "        self.a = np.zeros(X.shape[0])\n",
    "        for iteration in range(self.num_iterations):\n",
    "            \n",
    "            for i, x_i, y_i in zip(range(X.shape[0]),X, y):\n",
    "                y_hat = self.predict(y, X, x_i)\n",
    "                if y_hat != y_i:\n",
    "                    # START TODO\n",
    "\n",
    "                    # END TODO\n",
    "    def score(self, X, y):\n",
    "        # START TODO\n",
    "\n",
    "        # END TODO\n",
    "        accuracy = correct / X.shape[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now complete the code for the `rbf_kernel` implementation with a $\\gamma$ = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x, z):\n",
    "    return x.T@z\n",
    "def rbf_kernel(x,z):\n",
    "    # START TODO\n",
    "\n",
    "    # END TODO\n",
    "\n",
    "clf = MyKernelPerceptron(rbf_kernel, 3)\n",
    "\n",
    "clf.fit(X_with_ones, y)\n",
    "print(\"Accuracy of MyKernelPerceptron with rbf kernel\", clf.score(X_with_ones,y))\n",
    "\n",
    "clf = MyKernelPerceptron(linear_kernel, 3)\n",
    "\n",
    "clf.fit(X_with_ones, y)\n",
    "print(\"Accuracy of MyKernelPerceptron with linear kernel\", clf.score(X_with_ones,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions.  \n",
    "* **Which perceptron performed better?**\n",
    "* **Which was faster to run, MyKernelPerceptron or MyPerceptron?**\n",
    "* **When would it be preferable to use MyKernelPerceptron on the original data over MyPerceptron on lifted data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
