{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kev:kernels_project_unsolved.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK-3jbuLje0W"
      },
      "source": [
        "## Question 1) Nonlinear Classification Boundaries with Kernels - Kevin\n",
        "In your discussion of SVM's, we discussed the idea of **linear separability** for our input data and we also saw how featurizing helps when this is not the case. In this problem, we're going to take a look at decision boundaries that may be nonlinear, and how kernels can help us classify datasets where such decision boundaries best capture our underlining data features. We will explore and compare the following kernels; Linear, Polynomial, Sigmoid, RBF, and Rotational Quadratic. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_JnXnNokyio"
      },
      "source": [
        "### Question 1a)\n",
        "For the purposes of this problem we will be working with the sklearn built-in half moons dataset. Run the code below to load this dataset and create a scatter-plot from this data. Describe whether this data appears linear seperable or not. Can we expect good performance from a linear classification boundary using SVM?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GQIW1VAje-C"
      },
      "source": [
        "# IMPORT LIBRARIES\n",
        "from sklearn import datasets, svm\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "samples = 300\n",
        "X, Y = datasets.make_moons(samples)\n",
        "X_train = X[:200]\n",
        "Y_train = Y[:200]\n",
        "X_test = X[200:]\n",
        "Y_test = Y[200:]\n",
        "\n",
        "# Make a Pandas style dataframe:\n",
        "df = {'x1': X_train[:,0], 'x2': X_train[:,1], 'y': Y_train}\n",
        "# Scatterplot:\n",
        "sns.relplot(data=df, x='x1', y='x2', hue='y', kind='scatter')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DslJgKWk9xNB"
      },
      "source": [
        "**Include your answer here...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o54nNkZ1qB0b"
      },
      "source": [
        "### Question 1b)\n",
        "Lets set a baseline for our classification by using SVM with a soft-max margin and a linear decision boundary. Fill in the code below to perform the classification, create the visualization of the decision boundary along with the support vectors, and report the classification accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RnZIfnDjhZo"
      },
      "source": [
        "# 1b(i) fill in this method for an SVM with a linear fit of our data\n",
        "def fitLinearModel(X, Y):\n",
        "    '''\n",
        "    input: X is input data, and Y the corresponding labels \n",
        "    output: svm.SVC object with a linear fit of our data\n",
        "    '''\n",
        "    clf = ... # your code here\n",
        "    # ---- answer\n",
        "    clf = svm.SVC(kernel='linear')\n",
        "    clf.fit(X, Y)\n",
        "    # ----\n",
        "    return clf\n",
        "\n",
        "# 1b(ii) fill in this method to create the visualizations\n",
        "def drawSVM(clf, fignum=1):\n",
        "    '''\n",
        "    input: clf is an svm.SVC object, and fignum the current figure number.\n",
        "    output: svm.SVC object with a linear classification\n",
        "    '''\n",
        "    # plot the line, the points, and the nearest vectors to the plane\n",
        "    plt.figure(fignum, figsize=(8, 6))\n",
        "    plt.clf()\n",
        "\n",
        "    sv_x1 = ... # x1 coordinates of the support vectors, your code here\n",
        "    sv_x2 = ... # x2 coordinates of the support vectors, your code here\n",
        " \n",
        "    plt.scatter(sv_x1, sv_x2, \n",
        "                s=80, facecolors='none', zorder=10, edgecolors='k')\n",
        "    plt.scatter(df['x1'], df['x2'], c=df['y'], \n",
        "                zorder=10, cmap=plt.cm.Paired, edgecolors='k')\n",
        "    x_min, x_max = -3, 3\n",
        "    y_min, y_max = -3, 3\n",
        "\n",
        "    # Create 200x200 uniform samples across [x_min, x_max]*[y_min, y_max]\n",
        "    xy_samples = ... # Your code here\n",
        "\n",
        "    Z = clf.decision_function(xy_samples)\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(XX.shape)\n",
        "    plt.figure(fignum, figsize=(8, 6))\n",
        "    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
        "    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n",
        "                levels=[-.5, 0, .5])\n",
        "\n",
        "\n",
        "clf = fitLinearModel(X_train, Y_train)\n",
        "drawSVM(clf)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRxL66r3VV6T",
        "outputId": "31e84e97-5c05-4ba6-c3fe-e5c919d303ee"
      },
      "source": [
        "# 1b(iii) Fill in the code to get your classification accuracy based on the test data\n",
        "score = ...\n",
        "print(\"Linear boundary SVM classification average accuracy: \", score)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear boundary SVM classification average accuracy:  Ellipsis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS9nernhhXii"
      },
      "source": [
        "### Question 1c)\n",
        "We have seen the SVM classify our dataset by drawing a linear decision boundary, and we saw how our data naturally expresses itself. We would like to explore whether this natural expression produces more accurate classifications under a non-linear decision boundary. For this part of the question, we will explore a polynomial decision boundary using a degree d polynomial kernel function. Fill in the code below to perform the classification under different d degree values. Find and report the optimal degree d choice for which the best classification accuracy is obtained. Finally, re-use your drawing method from the previous part to create a visualization of the decision boundary along with the support vectors in-use. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKhz11fsjkgH"
      },
      "source": [
        "degrees = list(range(2, 7))\n",
        "# 1c(i) fill in this method for an SVM with a polynomial fit of our data\n",
        "def fitLinearModel(X, Y, d):\n",
        "    '''\n",
        "    input : X is input data, Y the corresponding labels, and \n",
        "            d is the degree of the polynomial kernel function\n",
        "    output:  svm.SVC object fitting X, Y using a polynomial kernel\n",
        "    '''\n",
        "    clf = ... # your code here\n",
        "    return clf\n",
        "\n",
        "# 1c(ii) find in this method that returns the svm.SVC object along with\n",
        "# the degree choice and classification results\n",
        "def chooseBest(X, Y):\n",
        "    '''\n",
        "    input : X is input data, and Y the corresponding labels\n",
        "    output: degree associated with the highest accuracy,\n",
        "            svm.SVC object with best degree choice polynomial classification\n",
        "            accuracy achoived the svm on our dataset\n",
        "    '''\n",
        "    return best_degree, best_svm, best_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toq3czZnjm7G"
      },
      "source": [
        "# 1c(iii) Create the visualization of this decision boundary and support vectors\n",
        "# report the classification average accuracy\n",
        "\n",
        "print(d, \"-degree Polynomial boundary SVM with classification average accuracy: \", score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvGciGmuijUu"
      },
      "source": [
        "### Question 1d)\n",
        "Fill in the code below to perform the classification using the Sigmoid kernel. Draw the decision boundary found by SVM with this kernel and report the classification accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrIdzM9Hij5T"
      },
      "source": [
        "# Note, you may want to consider the figure's axis to choose C\n",
        "# 1d(i) fill in this method for an SVM with a RBF fit of our data\n",
        "def fitSigmoidModel(X, Y, C=.1, gamma='scale'):\n",
        "    '''\n",
        "    input : X is input data, and Y the corresponding labels\n",
        "    output: svm.SVC object fitting X, Y using a Sigmoid kernel\n",
        "    '''\n",
        "    clf = ... # your code here\n",
        "    return clf\n",
        "\n",
        "clf = fitSigmoidModel(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ-cYmxGjo0Z"
      },
      "source": [
        "# 1d(ii) Create the visualization of this decision boundary and support vectors\n",
        "# report the classification average accuracy\n",
        "score = ...\n",
        "print(\"Sigmoid Kerneled SVM with classification average accuracy: \", score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFfLJm71ij5T"
      },
      "source": [
        "### Question 1e)\n",
        "We have now seen a linear decision boundary, a polynomial decision boundary, and a sigmoid kernel induced decision boundary. Alternatively, we can consider the performance of the RBF kernel in this dataset and the decision boundary induced by this. \n",
        "\n",
        "Fill in the code below to perform the classification using the RBF kernel. Draw the decision boundary found by SVM with this kernel and report the classification accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9FK4WDWijUv"
      },
      "source": [
        "# 1e(i) fill in this method for an SVM with a RBF fit of our data\n",
        "def fitRBFModel(X, Y):\n",
        "    '''\n",
        "    input : X is input data, and Y the corresponding labels\n",
        "    output: svm.SVC object fitting X, Y using a RBF kernel\n",
        "    '''\n",
        "    clf = ... \n",
        "    return clf\n",
        "\n",
        "clf = fitRBFModel(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL9cBQ3Tjr4Q"
      },
      "source": [
        "# 1e(ii) Create the visualization of this decision boundary and support vectors\n",
        "# report the classification average accuracy\n",
        "\n",
        "score = ...\n",
        "print(\"RBF Kerneled SVM with classification average accuracy: \", score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wXzMrE6gOw8"
      },
      "source": [
        "### Question 1f)\n",
        "We have now exhausted the default carried kernels in Scikits SVM. This does not mean, however, we do not have more Kernel functions to test. For exploratory purposes, this part of the question will guide you through introducing alternative kernels to SVM. We will have to take a slightly different approach when using SVM as custom kernels produce different behavior. For example, we will not have our support_vector filled automatically. We will also not have the Gram Matrix precomputed for us, and so we will have to do this for our data matrix and for any prediction. For your reference, we have included an implementation for the Gaussian Kernel:\n",
        "\n",
        "$$ K(x, y) = \\exp{(\\frac{-||x-y||^2}{2\\sigma^2})} $$\n",
        "\n",
        "\n",
        "Fill in the code below to implement the Rotational Quadratic Kernel:\n",
        "$$ K(x, y) = 1 - \\frac{||x-y||^2}{||x-y||^2+c} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJDwo8ViciNN"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import svm\n",
        "\n",
        "# 1f(i) fill in this method to compute the Gram matrix based on a custom kernel.\n",
        "def getGM(X1, X2, kernel):\n",
        "    \"\"\"\n",
        "    input: \n",
        "    output: Return the Gram matrix with the kernel evaluated on the pairs of\n",
        "            rows from X1, X2\n",
        "    \"\"\"\n",
        "    return gram_matrix\n",
        "\n",
        "# As an example here is the Gaussian Kernel\n",
        "def gaussKernel(x1, x2, sigma=.1):\n",
        "  return np.exp(- np.sum( np.power((x1 - x2),2) ) / float( 2*(sigma**2) ) )\n",
        "\n",
        "# 1f(ii) fill in the method below to evaluate the rotational quadratic kernel\n",
        "def rot_quad_kernel(x1, x2, c=0.01):\n",
        "  # your code here\n",
        "  pass\n",
        "\n",
        "clf = svm.SVC(kernel=\"precomputed\")\n",
        "\n",
        "# 1f(iii) fill in the lines below to fit the data using our kernel choice.\n",
        "# Hint: use the Gram matrix as you would a featurize matrix."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSVqvZwyT9Pg"
      },
      "source": [
        "We can no longer reuse our previous drawing methods because we need to make sure we transform according to our kernel. Thus, the data matrix we should use to fit should be comprised of evaluating all pairs of rows from our original matrix. Similarly, any predictions should also be transformed to consist of kernel evaluations of the new data points and the original train data points. Note, it may take a while to load the drawing method since we are not necessarily requiring optimized runtimes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBlWLWXljvX5"
      },
      "source": [
        "# 1f(iii) Modify the previous draw function so that it works with SVM\n",
        "def drawSVMK(clf, getGram, fignum=1):\n",
        "    '''\n",
        "    input: clf is an svm.SVC object, and fignum the current figure number.\n",
        "    output: svm.SVC object with a linear classification\n",
        "    '''\n",
        "    # plot the line, the points, and the nearest vectors to the plane\n",
        "    plt.figure(fignum, figsize=(8, 6))\n",
        "    plt.clf()\n",
        "    plt.scatter(df['x1'], df['x2'], c=df['y'], \n",
        "                zorder=10, cmap=plt.cm.Paired, edgecolors='k')\n",
        "    x_min, x_max = -3, 3\n",
        "    y_min, y_max = -3, 3\n",
        "\n",
        "    # Create 200x200 uniform samples across [x_min, x_max]*[y_min, y_max]\n",
        "    xy_samples = ... # Your code here (can reuse as before)\n",
        "\n",
        "    # Recall, we have to make use of our kernel to build a new Gram matrix\n",
        "    # for our inputs. Uncomment and fill in to sample our decision boundary\n",
        "\n",
        "    # Z = clf.predict(your_code_here)\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(XX.shape)\n",
        "    plt.figure(fignum, figsize=(8, 6))\n",
        "    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
        "    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n",
        "                levels=[-.5, 0, .5])\n",
        "\n",
        "drawSVMK(clf, myGM)\n",
        "score = clf.score(myGM(X_test, X_train), Y_test)\n",
        "# ---\n",
        "print(\"My Kerneled SVM with classification average accuracy: \", score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXFdwAhhkQ1f"
      },
      "source": [
        "### Question 1g)\n",
        "Rank the SVM performance under the different kernel choices and explain how these boundaries affected our classification accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fjIjpzejqYU"
      },
      "source": [
        "** Your answer here **"
      ]
    }
  ]
}